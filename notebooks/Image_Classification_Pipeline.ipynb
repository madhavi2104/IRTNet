{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Pipeline\n",
    "This notebook implements an image classification pipeline using multiple models, including torchvision, timm, huggingface, and openclip.\n",
    "It processes a dataset of images and generates predictions, saving the results in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Thesis\\IRTNet\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports and Dependencies\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForImageClassification\n",
    "from timm import create_model, list_models\n",
    "import open_clip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configurations\n",
    "Define the configurations for all models used in this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"resnet50\": {\"loader\": \"torchvision\", \"accuracy\": 79.0},\n",
    "    \"densenet121\": {\"loader\": \"torchvision\", \"accuracy\": 74.9},\n",
    "    \"efficientnet_b0\": {\"loader\": \"torchvision\", \"accuracy\": 77.7},\n",
    "    \"vgg16\": {\"loader\": \"torchvision\", \"accuracy\": 71.6},\n",
    "    \"mobilenet_v3_large\": {\"loader\": \"torchvision\", \"accuracy\": 73.0},\n",
    "    \"alexnet\": {\"loader\": \"torchvision\", \"accuracy\": 57.2},\n",
    "    \"coca_ViT-L-14\": {\n",
    "        \"loader\": \"openclip\",\n",
    "        \"model_name\": \"coca_ViT-L-14\",\n",
    "        \"pretrained_tag\": \"mscoco_finetuned_laion2b_s13b_b90k\",\n",
    "        \"accuracy\": 75.6,\n",
    "    },\n",
    "    \"clip_resnet50x4\": {\n",
    "        \"loader\": \"openclip\",\n",
    "        \"model_name\": \"RN50x4\",\n",
    "        \"pretrained_tag\": \"openai\",\n",
    "        \"accuracy\": 73.5,\n",
    "    },\n",
    "    \"beit_v2_base\": {\n",
    "        \"loader\": \"huggingface\",\n",
    "        \"model_name\": \"microsoft/beit-base-patch16-224-pt22k-ft22k\",\n",
    "        \"accuracy\": 74.9,\n",
    "    },\n",
    "    \"vit_base_patch16_224\": {\"loader\": \"timm\", \"model_name\": \"vit_base_patch16_224\", \"accuracy\": 69.1},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Function\n",
    "This function loads models based on their specified loader type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, loader, model_specific_name=None, config=None):\n",
    "    \"\"\"Load a model based on the loader type.\"\"\"\n",
    "    try:\n",
    "        if loader == \"torchvision\":\n",
    "            model = getattr(models, model_name)(weights=\"DEFAULT\")\n",
    "        elif loader == \"huggingface\":\n",
    "            model = AutoModelForImageClassification.from_pretrained(model_specific_name)\n",
    "        elif loader == \"timm\":\n",
    "            model = create_model(model_specific_name, pretrained=True)\n",
    "        elif loader == \"openclip\":\n",
    "            pretrained_tag = config.get(\"pretrained_tag\", \"openai\")\n",
    "            model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "                model_specific_name, pretrained=pretrained_tag\n",
    "            )\n",
    "            return model.eval(), preprocess\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loader type: {loader}\")\n",
    "        return model.eval(), None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_name}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Models\n",
    "Iterate through the model configurations and load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Thesis\\IRTNet\\.venv\\Lib\\site-packages\\open_clip\\factory.py:372: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "models_dict = {}\n",
    "preprocess_dict = {}\n",
    "for name, config in model_configs.items():\n",
    "    model, preprocess = load_model(name, config[\"loader\"], config.get(\"model_name\"), config)\n",
    "    if model is None:\n",
    "        print(f\"Skipping {name}: Model not loaded.\")\n",
    "        continue\n",
    "    models_dict[name] = model\n",
    "    if preprocess:\n",
    "        preprocess_dict[name] = preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function\n",
    "Defines a function to predict the class of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def get_prediction(image_path, model, preprocess=None):\n",
    "    \"\"\"Get model prediction for a single image.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        input_tensor = (preprocess(image) if preprocess else default_transform(image)).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            \n",
    "            if isinstance(output, torch.Tensor):  # Standard case\n",
    "                return output.argmax(dim=1).item()\n",
    "            elif hasattr(output, \"logits\"):  # Huggingface-style\n",
    "                return output.logits.argmax(dim=1).item()\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported output type: {type(output)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
